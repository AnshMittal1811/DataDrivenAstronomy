{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data galaxies: 780\n",
      "Train fraction: 0.7\n",
      "Number of galaxies in training set: 546\n",
      "Number of galaxies in testing set: 234\n"
     ]
    }
   ],
   "source": [
    "# # Introduction\n",
    "\n",
    "# In this activity, we will be using machine learning to classify galaxies into three types (ellipticals, spirals or galactic mergers) based on their observed properties.\n",
    "# In the last activity you had a go at classifying galaxies by hand on the Galaxy Zoo website which hopefully gave you some intuition for the dataset and how we can distinguish between the different types of galaxy.\n",
    "# For our machine learning experiments, we are using the crowd-classified classes from Galaxy Zoo as the training data for our automatic decision tree classifier.\n",
    "# We'll start by looking at how classification differs from regression. We will then implement some of the new features and parameters that we will use to reduce the dimensionality of the problem. We will also show you how to measure accuracy for classification problems, before extending our classifier to use random forests.\n",
    "# If you would like to try this on your own machine, the data set can be downloaded here.\n",
    "\n",
    "\n",
    "\n",
    "# # Classification vs Regression\n",
    "\n",
    "# In classification, the predictions are from a fixed set of classes, whereas in regression the prediction typically corresponds to a continuum of possible values.\n",
    "# In regression, we measure accuracy by looking at the size of the differences between the predicted values and the actual values. In contrast, in classification problems a prediction can either be correct or incorrect. This makes measuring the accuracy of our model a lot simpler.\n",
    "# In terms of implementation using decision trees, there is very little difference between classification and regression. The only notable difference is that our targets are classes rather than real values. When calculating the accuracy, we check whether the predicted class matches as the actual class.\n",
    "# A note on decision tree regression\n",
    "# In decision tree regression, the possible outputs are a finite set of values that correspond to the number of leaves/end points in the tree. Ideally we want as many points as possible to give a good approximation of the 'continuous' parameter space, whilst avoiding overfitting.\n",
    "\n",
    "\n",
    "\n",
    "# # The Galaxy Zoo data\n",
    "\n",
    "# In the last activity, you were a human classifier for the Galaxy Zoo project and probably saw a wide range of galaxy types observed by the Sloan Digital Sky Survey. In this activity, we will limit our dataset to three types of galaxy: spirals, ellipticals and mergers.\n",
    "\n",
    "# Galaxy Classes\n",
    "# The three galaxy classes. Click to see full size.\n",
    "# https://groklearning-cdn.com/modules/VGYTN65op4LW9u2ZGqGQSL/Classes.png\n",
    "\n",
    "# The galaxy catalogue we are using is a sample of galaxies where at least 20 human classifiers (such as yourself) have come to a consensus on the galaxy type.\n",
    "# Examples of spiral and elliptical galaxies were selected where there was a unanimous classification. Due to low sample numbers, we included merger examples where at least 80% of human classifiers selected the merger class.\n",
    "# We need this high quality data to train our classifier.\n",
    "\n",
    "\n",
    "\n",
    "# # Deciding on Features\n",
    "\n",
    "# Just like in the regression activities, we need to decide on a set of key features that represent our data.\n",
    "# While approaches exist that determine their own feature representation and use the raw pixel values as inputs, e.g. neural networks and deep learning, the majority of existing machine learning in astronomy requires an expert to design the feature set.\n",
    "# In this activity we will be using a set of features derived from fitting images according to known galaxy profiles.\n",
    "# Most of the features we use here are based on the five observed flux magnitudes from the Sloan Digital Sky Survey filters:\n",
    "\n",
    "# https://groklearning-cdn.com/modules/y8uy8PqJcjpPE47SKB3pPA/sdss_filters.png\n",
    "\n",
    "\n",
    "\n",
    "# # Selecting Features\n",
    "\n",
    "# The features that we will be using to do our galaxy classification are colour index, adaptive moments, eccentricities and concentrations. These features are provided as part of the SDSS catalogue.\n",
    "# We briefly describe these below. Further information how they are calculated can be found here.\n",
    "# Colour indices are the same colours (u-g, g-r, r-i, and i-z) we used for regression. Studies of galaxy evolution tell us that spiral galaxies have younger star populations and therefore are 'bluer' (brighter at lower wavelengths). Elliptical galaxies have an older star population and are brighter at higher wavelengths ('redder').\n",
    "# Eccentricity approximates the shape of the galaxy by fitting an ellipse to its profile. Eccentricity is the ratio of the two axis (semi-major and semi-minor). The De Vaucouleurs model was used to attain these two axis. To simplify our experiments, we will use the median eccentricity across the 5 filters.\n",
    "\n",
    "# https://groklearning-cdn.com/modules/ZBL7wT9VqgcJZUBe2WDqva/eccentricity_example.png\n",
    "\n",
    "\n",
    "# # Adaptive moments also describe the shape of a galaxy. They are used in image analysis to detect similar objects at different sizes and orientations. We use the fourth moment here for each band.\n",
    "\n",
    "# Concentration is similar to the luminosity profile of the galaxy, which measures what proportion of a galaxy's total light is emitted within what radius. A simplified way to represent this is to take the ratio of the radii containing 50% and 90% of the Petrosian flux.\n",
    "# The Petrosian method allows us to compare the radial profiles of galaxies at different distances. If you are interested, you can read more here on the need for Petrosian approach.\n",
    "# For these experiments, we will define concentration as:\n",
    "#   conc = petro (R50) / petro (R90)\n",
    "# We will use the concentration from the u, r and z bands.\n",
    "\n",
    "# https://groklearning-cdn.com/modules/UAAhKfyAhDnQWiVUP5NMi6/concentration_example.png\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Data Description\n",
    "\n",
    "# We have extracted the SDSS and Galaxy Zoo data for 780 galaxies into a NumPy binary file. You can load the file using:\n",
    "\n",
    "# import numpy as np\n",
    "# data = np.load('galaxy_catalogue.npy')\n",
    "\n",
    "# The data is a NumPy array of 780 records. Each record is a single galaxy. You can access the columns using field names. For example, to access the u-g colour filter for the first galaxy, you use:\n",
    "\n",
    "# data[0]['u-g']\n",
    "\n",
    "# Here's a snippet that prints the field and values for the first galaxy:\n",
    "\n",
    "# import numpy as np\n",
    "# data = np.load('galaxy_catalogue.npy')\n",
    "# for name, value in zip(data.dtype.names, data[0]):\n",
    "#   print('{:10} {:.6}'.format(name, value))\n",
    "\n",
    "# It shows the four colour features, median eccentricity, fourth adaptive moment of each filter, the Petrosian fluxes at a radius of 50% and 90% for the u, r and z filters, and finally the class:\n",
    "\n",
    "# u-g        1.85765\n",
    "# g-r        0.67158\n",
    "# r-i        0.4231\n",
    "# i-z        0.3061\n",
    "# ecc        0.585428\n",
    "# m4_u       2.25195\n",
    "# m4_g       2.33985\n",
    "# m4_r       2.38065\n",
    "# m4_i       2.35974\n",
    "# m4_z       2.39553\n",
    "# petroR50_u 3.09512\n",
    "# petroR50_r 3.81892\n",
    "# petroR50_z 3.82623\n",
    "# petroR90_u 5.17481\n",
    "# petroR90_r 8.26301\n",
    "# petroR90_z 11.4773\n",
    "# class      merger\n",
    "\n",
    "# NumPy also allows you to access a field for all of the rows at once, i.e. a column, using the field's name:\n",
    "# data['u-g']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Assignment :  Splitting the train and test sets\n",
    "\n",
    "# To start, we need to split the data into training and testing sets.\n",
    "# Your task is to implement the splitdata_train_test function. It takes a NumPy array and splits it into a training and testing NumPy array based on the specified training fraction. The function takes two arguments and should return two values:\n",
    "\n",
    "# Arguments\n",
    "# * data: the NumPy array containing the galaxies in the form described in the previous slide;\n",
    "# * fraction_training: the fraction of the data to use for training. This will be a float between 0 and 1.\n",
    "\n",
    "# The number of training rows should be truncated if necessary. For example, with a fraction of 0.67 and our 780 galaxies, the number of training rows is 780*0.67 = 722.6, which should be truncated to 722 using int. The remaining rows should be used for testing.\n",
    "\n",
    "# Return values\n",
    "# * training_set: the first value is a NumPy array training set;\n",
    "# * testing_set: the second value is a NumPy array testing set.\n",
    "\n",
    "# Using the supplied driver code, and our input data and a fraction of 0.7, the program should print the following values:\n",
    "\n",
    "# Number data galaxies: 780\n",
    "# Train fraction: 0.7\n",
    "# Number of galaxies in training set: 546\n",
    "# Number of galaxies in testing set: 234\n",
    "\n",
    "# Good practice: randomize the dataset order\n",
    "# You shouldn't assume that the data has already been shuffled. If you look at data['class'] you will see that the merger, elliptical and spiral examples appear together. Failing to shuffle the data will produce a very bad classifier! You can use:\n",
    "\n",
    "# np.random.seed(0)\n",
    "# np.random.shuffle(data)\n",
    "\n",
    "# The first statement ensures the shuffle is the same for each experiment, so you get consistent results, the second shuffles the rows of the data array in place.\n",
    "\n",
    "\n",
    "# # Assignment Code\n",
    "\n",
    "# Here we define a split_index that can be used to seperate the data array into two the training and testing sets respectively.\n",
    "# The split_index is determined by multiplying the fraction_training and multiplying it by the total number of data points. Converting to an integer will automatically truncate any fractional component of the floating point value.\n",
    "# We also use np.random.shuffle to shuffle the position of the indexes. This is not strictly required, but it is always a good idea to prevent selection bias.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def splitdata_train_test(data, fraction_training):\n",
    "  # complete this function\n",
    "  np.random.shuffle(data)\n",
    "  split_index = int(fraction_training*len(data))\n",
    "  return data[:split_index], data[split_index:]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  data = np.load('galaxy_catalogue.npy')\n",
    "\n",
    "  # set the fraction of data which should be in the training set\n",
    "  fraction_training = 0.7\n",
    "\n",
    "  # split the data using your function\n",
    "  training, testing = splitdata_train_test(data, fraction_training)\n",
    "\n",
    "  # print the key values\n",
    "  print('Number data galaxies:', len(data))\n",
    "  print('Train fraction:', fraction_training)\n",
    "  print('Number of galaxies in training set:', len(training))\n",
    "  print('Number of galaxies in testing set:', len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (780, 13)\n",
      "Targets shape: (780,)\n"
     ]
    }
   ],
   "source": [
    "# # Assignment: Generating features and targets\n",
    "\n",
    "# Next, we generate features and targets for the decision tree.\n",
    "# The generate_features_targets function is mostly complete. However, you need to calculate the concentration values for the u, r and z filters.\n",
    "# Your task is to calculate the concentration for each filter from the 50% and 90% Petrosian radius measurements:\n",
    "\n",
    "#     conc = petro (R50) / petro (R90)\n",
    "\n",
    "# As described earlier, data has the following fields:\n",
    "\n",
    "# *colours: u-g, g-r, r-i, and i-z;\n",
    "# *eccentricity: ecc\n",
    "# *4th adaptive moments: m4_u, m4_g, m4_r, m4_i, and m4_z;\n",
    "# *50% Petrosian: petroR50_u, petroR50_r, petroR50_z;\n",
    "# *90% Petrosian: petroR90_u, petroR90_r, petroR90_z.\n",
    "\n",
    "\n",
    "# # Assignment Code: \n",
    "\n",
    "# Here use the column names to calculate the concentration using the given formula, i.e. data['petroR50_*']/data['petroR90_*'] where the * represents each of the filters. These concentration values are populated straight into the input_features array.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_features_targets(data):\n",
    "  # complete the function by calculating the concentrations\n",
    "\n",
    "  targets = data['class']\n",
    "\n",
    "  features = np.empty(shape=(len(data), 13))\n",
    "  features[:, 0] = data['u-g']\n",
    "  features[:, 1] = data['g-r']\n",
    "  features[:, 2] = data['r-i']\n",
    "  features[:, 3] = data['i-z']\n",
    "  features[:, 4] = data['ecc']\n",
    "  features[:, 5] = data['m4_u']\n",
    "  features[:, 6] = data['m4_g']\n",
    "  features[:, 7] = data['m4_r']\n",
    "  features[:, 8] = data['m4_i']\n",
    "  features[:, 9] = data['m4_z']\n",
    "\n",
    "  # fill the remaining 3 columns with concentrations in the u, r and z filters\n",
    "  # concentration in u filter\n",
    "  features[:, 10] = (data['petroR50_u']/data['petroR90_u'])\n",
    "  # concentration in r filter\n",
    "#   features[:, 11] = (data['petroR50_r']/data['petroR90_r'])\n",
    "  # concentration in z filter\n",
    "  features[:, 12] = (data['petroR50_z']/data['petroR90_z'])\n",
    "\n",
    "  return features, targets\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  data = np.load('galaxy_catalogue.npy')\n",
    "\n",
    "  features, targets = generate_features_targets(data)\n",
    "\n",
    "  # Print the shape of each array to check the arrays are the correct dimensions. \n",
    "  print(\"Features shape:\", features.shape)\n",
    "  print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some initial results...\n",
      "   predicted,  actual\n",
      "0. merger, merger\n",
      "1. merger, merger\n",
      "2. elliptical, elliptical\n",
      "3. elliptical, elliptical\n",
      "4. spiral, spiral\n",
      "5. merger, spiral\n",
      "6. merger, spiral\n",
      "7. merger, spiral\n",
      "8. merger, merger\n",
      "9. merger, merger\n"
     ]
    }
   ],
   "source": [
    "# # Train the decision tree classifier\n",
    "\n",
    "# It is time to use the functions we wrote to split the data and generate the features, and then train a decision tree classifier.\n",
    "# Your task is complete the dtc_predict_actual function by following the Python comments. The purpose of the function is to perform a held out validation and return the predicted and actual classes for later comparison.\n",
    "# The function takes a single argument which is the full data set and should return two NumPy arrays containing the predicted and actual classes respectively.\n",
    "# You will also need to copy your solutions from the previous two questions into the spaces allocated.\n",
    "\n",
    "\n",
    "# # Assignment Code: \n",
    "\n",
    "# We first split the data into the our training and testing subsets using the earlier implementation of the splitdata_train_test.\n",
    "# Next we split the data further into train_features, train_targets and test_feautures, test_targets using the prior implementation of splitdata_features_targets\n",
    "# From there we follow the proceedure as with the regression decision trees. That is, we instantiate the decision tree with\n",
    "\n",
    "# dtc = DecisionTreeClassifier().\n",
    "\n",
    "# We train the tree with\n",
    "\n",
    "# dtc.fit(train_features, train_targets).\n",
    "\n",
    "# The predictions for the test set are determined using\n",
    "\n",
    "# predictions = dtc.predict(test_features),\n",
    "\n",
    "# before returning the predictions and their corresponding actual values (test_targets).\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# copy your splitdata_train_test function here\n",
    "def splitdata_train_test(data, fraction_training):\n",
    "  np.random.seed(0)\n",
    "  np.random.shuffle(data)  \n",
    "  split = int(len(data)*fraction_training)\n",
    "  return data[:split], data[split:]\n",
    "\n",
    "# copy your generate_features_targets function here\n",
    "def generate_features_targets(data):\n",
    "  # complete the function by calculating the concentrations\n",
    "\n",
    "  targets = data['class']\n",
    "\n",
    "  features = np.empty(shape=(len(data), 13))\n",
    "  features[:, 0] = data['u-g'] + 1\n",
    "  features[:, 1] = data['g-r']\n",
    "  features[:, 2] = data['r-i']\n",
    "  features[:, 3] = data['i-z']\n",
    "  features[:, 4] = data['ecc']\n",
    "  features[:, 5] = data['m4_u']\n",
    "  features[:, 6] = data['m4_g']\n",
    "  features[:, 7] = data['m4_r']\n",
    "  features[:, 8] = data['m4_i']\n",
    "  features[:, 9] = data['m4_z']\n",
    "\n",
    "  # fill the remaining 3 columns with concentrations in the u, r and z filters\n",
    "  # concentration in u filter\n",
    "  features[:, 10] = data['petroR50_u']/data['petroR90_u']\n",
    "  # concentration in r filter\n",
    "  features[:, 11] = data['petroR50_r']/data['petroR90_r']\n",
    "  # concentration in z filter\n",
    "  features[:, 12] = data['petroR50_z']/data['petroR90_z']\n",
    "\n",
    "  return features, targets\n",
    "\n",
    "\n",
    "# complete this function by splitting the data set and training a decision tree classifier\n",
    "def dtc_predict_actual(data):\n",
    "  # split the data into training and testing sets using a training fraction of 0.7\n",
    "  train, test = splitdata_train_test(data, 0.7)\n",
    "\n",
    "  # generate the feature and targets for the training and test sets\n",
    "  # i.e. train_features, train_targets, test_features, test_targets\n",
    "  train_features, train_targets = generate_features_targets(train)\n",
    "  test_features, test_targets = generate_features_targets(test)\n",
    "\n",
    "  # instantiate a decision tree classifier\n",
    "  dtc = DecisionTreeClassifier()\n",
    "\n",
    "  # train the classifier with the train_features and train_targets\n",
    "  dtc.fit(train_features, train_targets)\n",
    "\n",
    "  # get predictions for the test_features\n",
    "  predictions = dtc.predict(test_features)\n",
    "\n",
    "  # return the predictions and the test_targets\n",
    "  return predictions, test_targets\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  data = np.load('galaxy_catalogue.npy')\n",
    "    \n",
    "  predicted_class, actual_class = dtc_predict_actual(data)\n",
    "\n",
    "  # Print some of the initial results\n",
    "  print(\"Some initial results...\\n   predicted,  actual\")\n",
    "  for i in range(10):\n",
    "    print(\"{}. {}, {}\".format(i, predicted_class[i], actual_class[i]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our accuracy score: 0.7935897435897435\n",
      "Confusion matrix, without normalization\n",
      "[[199   5  56]\n",
      " [  4 234  22]\n",
      " [ 54  20 186]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Accuracy and Model Size\n",
    "\n",
    "# The accuracy of classification problems is a lot simpler to calculate than for regression problems. The simplest measure is the fraction of objects that are correctly classified. That is\n",
    "\n",
    "#     accuracy = (#correct predictions)/(#predictions)\n",
    "#     accuracy = ((summation from i=1 to n)predicted(i)=actual(i))/n\n",
    "        \n",
    "# The accuracy measure is often called the model score. While the way of calculating the score can vary depending on the model, the accuracy is the most common for classification problems.\n",
    "# Note: sklearn has methods to get the model score. Most models will have a score method which in the case of the decision tree classifier uses the above formula. The cross_val_score function in the model_selection module can be used to get k cross validated scores.\n",
    "\n",
    "\n",
    "\n",
    "# # Confusion Matrices\n",
    "\n",
    "# In addition to an overall accuracy score, we'd also like to know where our model is going wrong. For example, were the incorrectly classified mergers mis-classified as spirals or ellipticals? To answer this type of question we use a confusion matrix. An example confusion matrix for our problem is shown below:\n",
    "\n",
    "# Confusion Matrix\n",
    "# https://groklearning-cdn.com/modules/GK5JBj7XkSe3nTjuNGUFs9/example_confusion.png\n",
    "# A Confusion Matrix for ellipticals, mergers, and spirals. Click to enlarge.\n",
    "\n",
    "# The x axis represents the predicted classes and the y axis represents the correct classes. The value in each cell is the number of examples with those predicted and actual classes. Correctly classified objects are along the diagonal of the matrix.\n",
    "# So of the 260 actual spirals (correct class) in the data set, 198 are correctly predicted as spirals, 5 are incorrectly predicted as ellipticals and 57 are incorrectly predicted as mergers.\n",
    "# The sum along each row or column can be used to get the totals of true and predicted classes. So for example, by summing each of the rows we can confirm that there are 260 mergers, 260 spirals and 260 ellipticals in the data set.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Accuracy in Classification\n",
    "\n",
    "# Your task is to complete the calculate_accuracy function. The function should calculate the accuracy: the fraction of predictions that are correct (i.e. the model score):\n",
    "\n",
    "#         accuracy = (#correct predictions)/(#predictions)\n",
    "            \n",
    "# The function takes two arguments;\n",
    "# * predicted: an array of the predicted class for each galaxy.\n",
    "# * actual: an array of the actual class for each galaxy.\n",
    "\n",
    "# The return value should be a float (between 0 and 1).\n",
    "\n",
    "\n",
    "# # Assignment Code:\n",
    "\n",
    "# In this solution we use the builtin functions. Numpy allows array to array comparision using the standard comparison operators. The output is a new boolean array.\n",
    "# Boolean values are naturally mapped to 0 (False) and 1 (True), so we can sum the array to get the total number of correct predictions. From there it is simply a matter of comparing/dividing this with the total number of predictions.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from support_functions import plot_confusion_matrix, generate_features_targets\n",
    "import itertools\n",
    "\n",
    "def generate_features_targets(data):\n",
    "    output_targets = np.empty(shape=(len(data)), dtype='<U20')\n",
    "    output_targets[:] = data['class']\n",
    "\n",
    "    input_features = np.empty(shape=(len(data), 13))\n",
    "    input_features[:, 0] = data['u-g']\n",
    "    input_features[:, 1] = data['g-r']\n",
    "    input_features[:, 2] = data['r-i']\n",
    "    input_features[:, 3] = data['i-z']\n",
    "    input_features[:, 4] = data['ecc']\n",
    "    input_features[:, 5] = data['m4_u']\n",
    "    input_features[:, 6] = data['m4_g']\n",
    "    input_features[:, 7] = data['m4_r']\n",
    "    input_features[:, 8] = data['m4_i']\n",
    "    input_features[:, 9] = data['m4_z']\n",
    "    input_features[:, 10] = data['petroR50_u'] / data['petroR90_u']\n",
    "    input_features[:, 11] = data['petroR50_r'] / data['petroR90_r']\n",
    "    input_features[:, 12] = data['petroR50_z'] / data['petroR90_z']\n",
    "\n",
    "    return input_features, output_targets\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Greens):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{}\".format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Implement the following function\n",
    "def calculate_accuracy(predicted_classes, actual_classes):\n",
    "  return sum(predicted_classes == actual_classes)/len(actual_classes)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  data = np.load('galaxy_catalogue.npy')\n",
    "\n",
    "  # split the data\n",
    "  features, targets = generate_features_targets(data)\n",
    "\n",
    "  # train the model to get predicted and actual classes\n",
    "  dtc = DecisionTreeClassifier()\n",
    "  predicted = cross_val_predict(dtc, features, targets, cv=10)\n",
    "\n",
    "  # calculate the model score using your function\n",
    "  model_score = calculate_accuracy(predicted, targets)\n",
    "  print(\"Our accuracy score:\", model_score)\n",
    "\n",
    "  # calculate the models confusion matrix using sklearns confusion_matrix function\n",
    "  class_labels = list(set(targets))\n",
    "  model_cm = confusion_matrix(y_true=targets, y_pred=predicted, labels=class_labels)\n",
    "\n",
    "  # Plot the confusion matrix using the provided functions.\n",
    "  plt.figure()\n",
    "  plot_confusion_matrix(model_cm, classes=class_labels, normalize=False)\n",
    "  plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8628205128205129\n",
      "Confusion matrix, without normalization\n",
      "[[217   2  41]\n",
      " [  1 247  12]\n",
      " [ 36  15 209]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Random Forest Classification\n",
    "\n",
    "# So far we have used a single decision tree model. However, we can improve the accuracy of our classification by using a collection (or ensemble) of trees as known as a random forest.\n",
    "# A random forest is a collection of decision trees that have each been independently trained using different subsets of the training data and/or different combinations of features in those subsets.\n",
    "# When making a prediction, every tree in the forest gives its own prediction and the most common classification is taken as the overall forest prediction (in regression the mean prediction is used).\n",
    "\n",
    "\n",
    "\n",
    "# # Advantages of Random Forest Classifiers\n",
    "\n",
    "# Random forests help to mitigate overfitting in decision trees.\n",
    "# Training data is spread across decision trees. The subsets are created by taking random samples with replacement. This means that a given data point can be used in several subsets. (This is different from the subsets used in cross validation where each data point belongs to one subset).\n",
    "# Individual trees are trained with different subsets of features. So in our current problem, one tree might be trained using eccentricity and another using concentration and the 4th adaptive moment. By using different combinations of input features you create expert trees that are can better identify classes by a given feature.\n",
    "# The sklearn random forest only uses the first form of sampling.\n",
    "\n",
    "\n",
    "\n",
    "# # Assignment: Random Forest\n",
    "\n",
    "# Your task here is to complete the rf_predict_actual function. It returns the predicted and actual classes for our galaxies using a random forest 10-fold with cross validation.\n",
    "# You should use the RandomForestClassifier class from the sklearn.ensemble module. It can be instantiated with:\n",
    "\n",
    "# rfc = RandomForestClassifier(n_estimators=n_estimators)\n",
    "\n",
    "# n_estimators is the the number of decision trees in the forest.\n",
    "# rf_predict_actual takes two arguments: the data used throughout this activity and the number of estimators (n_estimators) to be used in the random forest.\n",
    "# The function should return two NumPy arrays containing the predicted and actual classes respectively.\n",
    "# You can copy and paste the functions from previous questions. However, we have provided the generate_features_targets function in the support library.\n",
    "# Use the cross_val_predict function from the model_selection module as we did in the last question.\n",
    "# You can read its documentation here. This approach allows us to get a prediction for every galaxy in the data set through cross validation. It also means that we don't need to manage the training and test sets.\n",
    "\n",
    "\n",
    "# # Assignment Codes:\n",
    "\n",
    "# This is the solution for the second of the two options. We start by splitting the data into features and targets using the splitdata_features_targets method from the support_functions. You could have alternatively have copied your own implementation from earlier.\n",
    "# The next step is to instantiate the random forest classifier. Note that we need to pass the n_estimators from the parent functions arguments (predictions_actuals_randomforest).\n",
    "# Once we have split the data and instantiated the random forest classifier we are able to call the cross_val_predict function. We pass the model (RandomForestClassifier) the features and targets to the function and specific an additional optional parameter cv=10 which is the number of folds to do in the cross validations.\n",
    "# We finally return the predictions from the cross validation and their corresponding target/actual values.\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Supporting Functions\n",
    "\n",
    "def calculate_accuracy(predicted_classes, actual_classes, ):\n",
    "    return sum(actual_classes[:] == predicted_classes[:]) / len(actual_classes)\n",
    "\n",
    "\n",
    "def generate_features_targets(data):\n",
    "    output_targets = np.empty(shape=(len(data)), dtype='<U20')\n",
    "    output_targets[:] = data['class']\n",
    "\n",
    "    input_features = np.empty(shape=(len(data), 13))\n",
    "    input_features[:, 0] = data['u-g']\n",
    "    input_features[:, 1] = data['g-r']\n",
    "    input_features[:, 2] = data['r-i']\n",
    "    input_features[:, 3] = data['i-z']\n",
    "    input_features[:, 4] = data['ecc']\n",
    "    input_features[:, 5] = data['m4_u']\n",
    "    input_features[:, 6] = data['m4_g']\n",
    "    input_features[:, 7] = data['m4_r']\n",
    "    input_features[:, 8] = data['m4_i']\n",
    "    input_features[:, 9] = data['m4_z']\n",
    "    input_features[:, 10] = data['petroR50_u'] / data['petroR90_u']\n",
    "    input_features[:, 11] = data['petroR50_r'] / data['petroR90_r']\n",
    "    input_features[:, 12] = data['petroR50_z'] / data['petroR90_z']\n",
    "\n",
    "    return input_features, output_targets\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Reds):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{}\".format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# complete this function to get predictions from a random forest classifier\n",
    "def rf_predict_actual(data, n_estimators):\n",
    "  # generate the features and targets\n",
    "  features, targets = generate_features_targets(data)\n",
    "\n",
    "  # instantiate a random forest classifier\n",
    "  rfc = RandomForestClassifier(n_estimators=n_estimators)\n",
    "  \n",
    "  # get predictions using 10-fold cross validation with cross_val_predict\n",
    "  predicted = cross_val_predict(rfc, features, targets, cv=10)\n",
    "\n",
    "  # return the predictions and their actual classes\n",
    "  return predicted, targets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Driver Function\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  data = np.load('galaxy_catalogue.npy')\n",
    "\n",
    "  # get the predicted and actual classes\n",
    "  number_estimators = 50              # Number of trees\n",
    "  predicted, actual = rf_predict_actual(data, number_estimators)\n",
    "\n",
    "  # calculate the model score using your function\n",
    "  accuracy = calculate_accuracy(predicted, actual)\n",
    "  print(\"Accuracy score:\", accuracy)\n",
    "\n",
    "  # calculate the models confusion matrix using sklearns confusion_matrix function\n",
    "  class_labels = list(set(actual))\n",
    "  model_cm = confusion_matrix(y_true=actual, y_pred=predicted, labels=class_labels)\n",
    "\n",
    "  # plot the confusion matrix using the provided functions.\n",
    "  plt.figure()\n",
    "  plot_confusion_matrix(model_cm, classes=class_labels, normalize=False)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Results Discussion\n",
    "\n",
    "# Did the random forest improve the accuracy of the model? The answer is yes – we see a substantial increase in accuracy. When we look at the 10-fold cross validation results, we see that the random forest systematically out performs a single decision tree:\n",
    "\n",
    "# Random Forest\tDecision Tree\n",
    "# Median score\t0.865\t0.775\n",
    "# Mean score\t0.867\t0.792\n",
    "# Standard deviation of scores\t0.036\t0.035\n",
    "\n",
    "# The random forest is around ~6-7% more accurate than a standard decision tree.\n",
    "# Below is a side by side comparision of the confusion matrices from our galaxy catalogues. The first showing the results for the random forest classifier and the second for the decision tree classifier. There are improvements accross the board with the biggest improvement (percentage) being between ellipticals and spirals.\n",
    "\n",
    "# https://groklearning-cdn.com/modules/4cNpN3nHrL8rz3kkSKErs3/forest_tree_comparison.png\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusions\n",
    "\n",
    "# In this activity we have used decision tree classifiers to identify a galaxies type by a selection of derived features (e.g. eccentricity and concentration).\n",
    "# We have learnt that asessing the models performance is a lot simpler with classification than it is with regression.\n",
    "# Confusion matrices can be a useful tool to help understand where our model is over and under preforming with respect to each class.\n",
    "# We started out with the goal of using decision trees to classify galaxies as one of three types. We found that we were able to achieve an accuracy of around 80% using decision trees which is very good when you consider the statistical accuracy of a random selection is 33% and the naive approach using only pixel values yielded ~64%.\n",
    "# We were able to improve on the accuracy of the decision tree classifier by using a selection of them in ensemble learning with a random forest.\n",
    "\n",
    "\n",
    "\n",
    "# # Congratulations, you've finished the final set of activities for the MOOC!\n",
    "# If you've still got questions about any of the content, head to the forums to discuss with your fellow learners.\n",
    "# Now head back to Coursera for the wrap up lecture. Please let us know if you have any feedback about the Grok Learning platform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
